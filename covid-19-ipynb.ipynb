{"cells":[{"metadata":{"code_folding":[],"trusted":true,"_uuid":"1c434f30fff12aea5c71e7006a582205e0ed3ddf"},"cell_type":"code","source":"import warnings                                  # `do not disturbe` mode\nwarnings.filterwarnings('ignore')\n\nimport numpy as np                               # vectors and matrices\nimport pandas as pd                              # tables and data manipulations\nimport matplotlib.pyplot as plt                  # plots\nimport seaborn as sns                            # more plots\n\nfrom dateutil.relativedelta import relativedelta # working with dates with style\nfrom scipy.optimize import minimize              # for function minimization\n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_excel('../input/covid-19/COVID-19-geographic-disbtribution-worldwide.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IND = df[df['countryterritoryCode'] == 'IND']\nAF = df[df['countryterritoryCode'] == 'AFG']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(IND['Cumulative_number_for_14_days_of_COVID-19_cases_per_100000'].isna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(AF['Cumulative_number_for_14_days_of_COVID-19_cases_per_100000'].isna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AF = AF.iloc[:249]\nIND = IND.iloc[:258]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IND = IND[['dateRep','Cumulative_number_for_14_days_of_COVID-19_cases_per_100000']]\nAF = AF[['dateRep','Cumulative_number_for_14_days_of_COVID-19_cases_per_100000']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IND.columns = ['time','X']\nAF.columns = ['time','X']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IND.to_csv('IND')\nAF.to_csv('AF')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IND = pd.read_csv('./IND', index_col=['time'], parse_dates=['time'])\nAF = pd.read_csv('./AF', index_col=['time'], parse_dates=['time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1d67db8570fdf260088bcd9085d73d8f6d6a735"},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(IND.X)\nplt.title('Cumulative_number_for_14_days_of_COVID-19_cases_per_100000')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(AF.X)\nplt.title('Cumulative_number_for_14_days_of_COVID-19_cases_per_100000')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f639764f539d617e4e31854a215758ab5e8dbf7"},"cell_type":"code","source":"# Importing everything from above\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"trusted":true,"_uuid":"02e298fc74603ac05bdc34c69ca82456be2059d7"},"cell_type":"code","source":"def moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\nmoving_average(IND.X, 24) # prediction for the last observed day (past 24 hours)","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0],"trusted":true,"_uuid":"05c627011b7a8b5431c5abacfbc4cca322715bec"},"cell_type":"code","source":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4775d01ad54f45cbb3770bd2e3a92d2d503f92f7"},"cell_type":"code","source":"plotMovingAverage(IND.X, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMovingAverage(AF.X, 4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"126dd8d923f859005e0cdd00243ae40cdb49638f"},"cell_type":"markdown","source":"Now let's try smoothing by the previous 12 hours."},{"metadata":{"trusted":true,"_uuid":"d7ffebb1a55d81c4e54a53c6e05a84bd081ec35e"},"cell_type":"code","source":"plotMovingAverage(IND.X, 12) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMovingAverage(AF.X, 12) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9588a96a1e692d998ef2d6376aabca8e61005c7b"},"cell_type":"code","source":"plotMovingAverage(IND.X, 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMovingAverage(AF.X, 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6c548f01bbc2d4969d8350533e13e8cdd5676f3"},"cell_type":"code","source":"plotMovingAverage(IND.X, 4, plot_intervals=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMovingAverage(AF.X, 4, plot_intervals=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_average(series, weights):\n    \"\"\"\n        Calculate weighter average on series\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series.iloc[-n-1] * weights[n]\n    return float(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_average(IND.X, [0.6, 0.3, 0.1])","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"trusted":true,"_uuid":"72a50f984572c6b64e9f7a710d63dfe9c5fdfe7a"},"cell_type":"code","source":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0],"trusted":true,"_uuid":"d9d2aa6b0bbb80e4a34061e09bb9fc595f4dca3c"},"cell_type":"code","source":"def plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(15, 7))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d453adbfb0176c209f045159cd674529adec09e"},"cell_type":"code","source":"plotExponentialSmoothing(IND.X, [0.3, 0.05])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotExponentialSmoothing(AF.X, [0.3, 0.05])","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0,20],"trusted":true,"_uuid":"40987e7da13b51809c39a550641f27a20bca7289"},"cell_type":"code","source":"\ndef double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result\n\ndef plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76925b3be43aac4bd276573358dee7fb9b3087c9"},"cell_type":"code","source":"plotDoubleExponentialSmoothing(IND.X, alphas=[0.9, 0.02], betas=[0.9, 0.02])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotDoubleExponentialSmoothing(AF.X, alphas=[0.9, 0.02], betas=[0.9, 0.02])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"191730d10bb6f117255bf7bf994490c6a18339fd"},"cell_type":"markdown","source":"## Triple exponential smoothing a.k.a. Holt-Winters\n\nWe've looked at exponential smoothing and double exponential smoothing. This time, we're going into _triple_ exponential smoothing.\n\nAs you could have guessed, the idea is to add a third component - seasonality. This means that we should not use this method if our time series is not expected to have seasonality. Seasonal components in the model will explain repeated variations around intercept and trend, and it will be specified by the length of the season, in other words by the period after which the variations repeat. For each observation in the season, there is a separate component; for example, if the length of the season is 7 days (a weekly seasonality), we will have 7 seasonal components, one for each day of the week.\n\nWith this, let's write out a new system of equations:\n\n$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n\n$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n\nThe intercept now depends on the current value of the series minus any corresponding seasonal component. Trend remains unchanged, and the seasonal component depends on the current value of the series minus the intercept and on the previous value of the component. Take into account that the component is smoothed through all the available seasons; for example, if we have a Monday component, then it will only be averaged with other Mondays. You can read more on how averaging works and how the initial approximation of the trend and seasonal components is done [here](http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm). Now that we have the seasonal component, we can predict not just one or two steps ahead but an arbitrary $m$ future steps ahead, which is very encouraging.\n\nBelow is the code for a triple exponential smoothing model, which is also known by the last names of its creators, Charles Holt and his student Peter Winters. Additionally, the Brutlag method was included in the model to produce confidence intervals:\n\n$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n\n$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n\n$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T},$$\n\nwhere $T$ is the length of the season, $d$ is the predicted deviation. Other parameters were taken from triple exponential smoothing. You can read more about the method and its applicability to anomaly detection in time series [here](http://fedcsis.org/proceedings/2012/pliks/118.pdf)."},{"metadata":{"code_folding":[0],"trusted":true,"_uuid":"30f17eded5a8506472e2c5de74fae2f64b4e0764"},"cell_type":"code","source":"class HoltWinters:\n    \n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    \n    # series - initial time series\n    # slen - length of a season\n    # alpha, beta, gamma - Holt-Winters model coefficients\n    # n_preds - predictions horizon\n    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n        return sum / self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBond = []\n        self.LowerBond = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBond.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBond.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBond.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBond.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"762323f8f2e6da9b1049f4f5b23c2c88eb49223a"},"cell_type":"markdown","source":"## Time series cross validation\n\nBefore we start building a model, let's first discuss how to estimate model parameters automatically.\n\nThere is nothing unusual here; as always, we have to choose a loss function suitable for the task that will tell us how closely the model approximates the data. Then, using cross-validation, we will evaluate our chosen loss function for the given model parameters, calculate the gradient, adjust the model parameters, and so on, eventually descending to the global minimum.\n\nYou may be asking how to do cross-validation for time series because time series have this temporal structure and one cannot randomly mix values in a fold while preserving this structure. With randomization, all time dependencies between observations will be lost. This is why we will have to use a more tricky approach in optimizing the model parameters. I don't know if there's an official name to this, but on [CrossValidated](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection), where one can find all answers but the Answer to the Ultimate Question of Life, the Universe, and Everything, the proposed name for this method is \"cross-validation on a rolling basis\".\n\nThe idea is rather simple -- we train our model on a small segment of the time series from the beginning until some $t$, make predictions for the next $t+n$ steps, and calculate an error. Then, we expand our training sample to $t+n$ value, make predictions from $t+n$ until $t+2*n$, and continue moving our test segment of the time series until we hit the last available observation. As a result, we have as many folds as $n$ will fit between the initial training sample and the last observation.\n\n<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"},{"metadata":{"_uuid":"2be317d3cbe839006c9a4c1611c58757be36df5b"},"cell_type":"markdown","source":"Now, knowing how to set up cross-validation, we can find the optimal parameters for the Holt-Winters model. Recall that we have daily seasonality in ads, hence the `slen=24` parameter."},{"metadata":{"code_folding":[2],"trusted":true,"_uuid":"e6ce256bc582fb273c00970cf485687698b967c0"},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n\ndef timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=3) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = HoltWinters(series=values[train], slen=slen, \n                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2260d7db277fb76486dddfbe3a027b30b6e1a28"},"cell_type":"markdown","source":"In the Holt-Winters model, as well as in the other models of exponential smoothing, there's a constraint on how large the smoothing parameters can be, each of them ranging from 0 to 1. Therefore, in order to minimize our loss function, we have to choose an algorithm that supports constraints on model parameters. In our case, we will use the truncated Newton conjugate gradient."},{"metadata":{"trusted":true,"_uuid":"addf32ff16c5aaa8fb54b3061c639d9f4c99b29f"},"cell_type":"code","source":"%%time\ndata = IND.X[:-20] # leave some data for testing\n\n# initializing model parameters alpha, beta and gamma\nx = [0, 0, 0] \n\n# Minimizing the loss function \nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_squared_error), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\n# Take optimal values...\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\n# ...and train the model with them, forecasting for the next 50 hours\nmodel = HoltWinters(data, slen = 24, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 50, scaling_factor = 3)\nmodel.triple_exponential_smoothing()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f4f7fa9e16e074712af7408e6e380d1e8050ae4"},"cell_type":"markdown","source":"Let's add some code to render plots."},{"metadata":{"code_folding":[0],"trusted":true,"_uuid":"f0ca23bc252bccb61d69383fa177972e54dd65de"},"cell_type":"code","source":"def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        series - dataset with timeseries\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    \n    plt.figure(figsize=(20, 10))\n    plt.plot(model.result, label = \"Model\")\n    plt.plot(series.values, label = \"Actual\")\n    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    \n    if plot_anomalies:\n        anomalies = np.array([np.NaN]*len(series))\n        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n            series.values[series.values<model.LowerBond[:len(series)]]\n        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n            series.values[series.values>model.UpperBond[:len(series)]]\n        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    if plot_intervals:\n        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n        \n    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n    plt.axvspan(len(series)-20, len(model.result), alpha=0.3, color='lightgrey')\n    plt.grid(True)\n    plt.axis('tight')\n    plt.legend(loc=\"best\", fontsize=13);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58ae37fb7525dd64dcfb3ba6529139ae6af8e430"},"cell_type":"code","source":"plotHoltWinters(IND.X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceb1ae36ebef543fc1846cc7d6231730b52c5560"},"cell_type":"code","source":"plotHoltWinters(IND.X, plot_intervals=True, plot_anomalies=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cb412247f45da7f4f147cef4c6504c5a297a765"},"cell_type":"code","source":"plt.figure(figsize=(25, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad5deafc34c2703abef9f5b81ed27c5bb2c86f86"},"cell_type":"markdown","source":"We'll apply the same algorithm for the second series which, as you may recall, has trend and a 30-day seasonality."},{"metadata":{"trusted":true,"_uuid":"d645f6852e5b4186c62eb841f32d79a47d112aba"},"cell_type":"code","source":"%%time\ndata = AF.X[:-20] \nslen = 4 # 30-day seasonality\n\nx = [0, 0, 0] \n\nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_absolute_percentage_error, slen), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\nmodel = HoltWinters(data, slen = slen, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 100, scaling_factor = 3)\nmodel.triple_exponential_smoothing()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ff07de84075be95c9895de6cefc01f3d4a13d13"},"cell_type":"code","source":"plotHoltWinters(AF.X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72c2e055aa4b016ce0606f688be06f35e210d3ac","collapsed":true},"cell_type":"code","source":"plotHoltWinters(AF.X, plot_intervals=True, plot_anomalies=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63c0d84ffc069cb79f06eaec43e4aa952174b407","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\");","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}